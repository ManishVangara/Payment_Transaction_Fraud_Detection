{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2638f8fe",
   "metadata": {},
   "source": [
    "# Payment Transaction Fraud Detection & Financial Monitoring\n",
    "\n",
    "**Personal Project | Analytics Engineering**\n",
    "\n",
    "---\n",
    "\n",
    "### Project Summary\n",
    "\n",
    "This notebook implements an end-to-end fraud detection system built on the **Sparkov synthetic transaction dataset** — 540,000+ transactions across 1,000 cardholders and 800 merchants. It demonstrates how behavioural anomaly detection outperforms static rule-based systems on a realistic payment dataset with fully interpretable features.\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Dataset** | Sparkov — `kaggle: kartik2112/fraud-detection` (CC0) |\n",
    "| **Fraud Rate** | 0.52% |\n",
    "| **Stack** | Python · SQL (PostgreSQL) · Tableau |\n",
    "| **Models** | Isolation Forest + Z-Score Anomaly Detection |\n",
    "| **Explainability** | SHAP per-alert attribution |\n",
    "\n",
    "### Key Results\n",
    "\n",
    "| Metric | Rules Baseline | Anomaly System | Change |\n",
    "|---|---|---|---|\n",
    "| Fraud Detection Rate | 54.6% | 83.1% | +28.5 ppt |\n",
    "| False Positive Rate | 22.3% | 14.5% | −35% relative |\n",
    "| Precision | 1.9% | 4.1% | +2.2 ppt |\n",
    "| F1 Score | 0.037 | 0.078 | +0.041 |\n",
    "| Alerts / 10K transactions | ~2,230 | ~1,450 | −35% |\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Structure\n",
    "\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [Data Loading & Exploration](#2-data-loading--exploration)\n",
    "3. [PostgreSQL Schema & Loading](#3-postgresql-schema--loading)\n",
    "4. [Feature Engineering (SQL + Python)](#4-feature-engineering)\n",
    "5. [Baseline: Rules-Based System](#5-baseline-rules-based-system)\n",
    "6. [Anomaly Detection Models](#6-anomaly-detection-models)\n",
    "7. [Model Evaluation](#7-model-evaluation)\n",
    "8. [SHAP Explainability](#8-shap-explainability)\n",
    "9. [Billing Reconciliation](#9-billing-reconciliation)\n",
    "10. [Tableau Dashboard Prep](#10-tableau-dashboard-prep)\n",
    "11. [Limitations & Next Steps](#11-limitations--next-steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef506057",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Install and import all required libraries. The dataset is available free from Kaggle — download `fraudTrain.csv` and `fraudTest.csv` and place them in `./data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3da770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install scikit-learn pandas numpy matplotlib seaborn shap scipy psycopg2-binary kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa638b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    precision_recall_curve, roc_curve, auc,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "import shap\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plot styling\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': '#f9f8f6',\n",
    "    'axes.facecolor': '#ffffff',\n",
    "    'axes.grid': True,\n",
    "    'grid.color': '#e5e3de',\n",
    "    'grid.linewidth': 0.8,\n",
    "    'font.family': 'sans-serif',\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "})\n",
    "\n",
    "print(\"All libraries imported successfully.\")\n",
    "print(f\"pandas  : {pd.__version__}\")\n",
    "print(f\"numpy   : {np.__version__}\")\n",
    "print(f\"sklearn : __import__('sklearn').__version__\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ad3d0",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Exploration\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "The **Sparkov Credit Card Transactions Fraud Detection Dataset** (`kartik2112/fraud-detection` on Kaggle) is synthetically generated using the Sparkov Data Generation tool. It contains fully interpretable fields — cardholder identity, merchant name, transaction category, amount, and geographic coordinates for both the cardholder and the merchant.\n",
    "\n",
    "This interpretability is what makes proper behavioural feature engineering possible. Unlike anonymised datasets (e.g. the ULB dataset where V1–V28 are PCA-transformed components), every feature we derive here is traceable to a real field with a real meaning.\n",
    "\n",
    "**Download instructions:**\n",
    "```bash\n",
    "kaggle datasets download -d kartik2112/fraud-detection\n",
    "unzip fraud-detection.zip -d ./data/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c63034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load raw data ──────────────────────────────────────────────────────────\n",
    "DATA_DIR = './data/'\n",
    "\n",
    "train_raw = pd.read_csv(os.path.join(DATA_DIR, 'fraudTrain.csv'))\n",
    "test_raw  = pd.read_csv(os.path.join(DATA_DIR, 'fraudTest.csv'))\n",
    "\n",
    "# Combine for EDA; we split chronologically later\n",
    "df_all = pd.concat([train_raw, test_raw], ignore_index=True)\n",
    "\n",
    "print(f\"Train rows : {len(train_raw):,}\")\n",
    "print(f\"Test rows  : {len(test_raw):,}\")\n",
    "print(f\"Total rows : {len(df_all):,}\")\n",
    "print(f\"Columns    : {list(df_all.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Schema preview ─────────────────────────────────────────────────────────\n",
    "df_all.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207cdc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Data types & nulls ─────────────────────────────────────────────────────\n",
    "print(\"Dtypes:\")\n",
    "print(df_all.dtypes)\n",
    "print(\"\\nNull counts:\")\n",
    "print(df_all.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Parse timestamp ────────────────────────────────────────────────────────\n",
    "df_all['trans_datetime'] = pd.to_datetime(df_all['trans_date_trans_time'])\n",
    "df_all = df_all.sort_values('trans_datetime').reset_index(drop=True)\n",
    "\n",
    "print(f\"Date range : {df_all['trans_datetime'].min()} → {df_all['trans_datetime'].max()}\")\n",
    "print(f\"Fraud cases: {df_all['is_fraud'].sum():,}  ({df_all['is_fraud'].mean()*100:.3f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e49111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── EDA: Fraud rate by merchant category ───────────────────────────────────\n",
    "cat_fraud = (\n",
    "    df_all.groupby('category')['is_fraud']\n",
    "    .agg(['mean', 'sum', 'count'])\n",
    "    .rename(columns={'mean': 'fraud_rate', 'sum': 'fraud_count', 'count': 'total'})\n",
    "    .sort_values('fraud_rate', ascending=False)\n",
    "    .reset_index()\n",
    ")\n",
    "cat_fraud['fraud_rate_pct'] = cat_fraud['fraud_rate'] * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.barh(cat_fraud['category'], cat_fraud['fraud_rate_pct'],\n",
    "               color='#d4380d', alpha=0.75, edgecolor='none')\n",
    "ax.set_xlabel('Fraud Rate (%)')\n",
    "ax.set_title('Fraud Rate by Merchant Category', fontweight='bold', pad=12)\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(cat_fraud[['category', 'fraud_rate_pct', 'fraud_count', 'total']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a92847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── EDA: Transaction amount distribution (fraud vs legitimate) ─────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "legit = df_all[df_all['is_fraud'] == 0]['amt'].clip(upper=500)\n",
    "fraud = df_all[df_all['is_fraud'] == 1]['amt'].clip(upper=500)\n",
    "\n",
    "axes[0].hist(legit, bins=60, color='#1a56a0', alpha=0.6, label='Legitimate')\n",
    "axes[0].hist(fraud, bins=60, color='#d4380d', alpha=0.7, label='Fraud')\n",
    "axes[0].set_title('Transaction Amount Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Amount (USD, capped at $500)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Hourly fraud pattern\n",
    "df_all['hour'] = df_all['trans_datetime'].dt.hour\n",
    "hourly = df_all.groupby('hour')['is_fraud'].mean() * 100\n",
    "axes[1].bar(hourly.index, hourly.values, color='#1a56a0', alpha=0.75)\n",
    "axes[1].set_title('Fraud Rate by Hour of Day', fontweight='bold')\n",
    "axes[1].set_xlabel('Hour (0–23)')\n",
    "axes[1].set_ylabel('Fraud Rate (%)')\n",
    "axes[1].yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd7208",
   "metadata": {},
   "source": [
    "## 3. PostgreSQL Schema & Loading\n",
    "\n",
    "The raw CSV is loaded into PostgreSQL 15 for feature engineering using SQL window functions. The schema below maps the Sparkov columns into a clean normalised table.\n",
    "\n",
    "> **Note:** Sections 3 and 4 show both the SQL approach (for production use) and a pure-pandas equivalent that runs directly in this notebook without a database connection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5571721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── PostgreSQL DDL (reference — run in psql or pgAdmin) ────────────────────\n",
    "psql_ddl = '''\n",
    "-- Create core transactions table\n",
    "CREATE TABLE IF NOT EXISTS transactions (\n",
    "    trans_id        VARCHAR(50) PRIMARY KEY,\n",
    "    trans_timestamp TIMESTAMP   NOT NULL,\n",
    "    cc_num          BIGINT      NOT NULL,\n",
    "    merchant        VARCHAR(200),\n",
    "    category        VARCHAR(100),\n",
    "    amount          NUMERIC(10,2),\n",
    "    cardholder_lat  NUMERIC(9,6),\n",
    "    cardholder_long NUMERIC(9,6),\n",
    "    merch_lat       NUMERIC(9,6),\n",
    "    merch_long      NUMERIC(9,6),\n",
    "    city            VARCHAR(100),\n",
    "    state           CHAR(2),\n",
    "    city_pop        INT,\n",
    "    is_fraud        SMALLINT DEFAULT 0\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_tx_cc_num    ON transactions(cc_num);\n",
    "CREATE INDEX IF NOT EXISTS idx_tx_timestamp ON transactions(trans_timestamp);\n",
    "CREATE INDEX IF NOT EXISTS idx_tx_category  ON transactions(category);\n",
    "\n",
    "-- Load from CSV\n",
    "COPY transactions (\n",
    "    trans_id, trans_timestamp, cc_num, merchant, category,\n",
    "    amount, cardholder_lat, cardholder_long, merch_lat, merch_long,\n",
    "    city, state, city_pop, is_fraud\n",
    ")\n",
    "FROM '/data/fraudTrain.csv'\n",
    "WITH (FORMAT csv, HEADER true);\n",
    "'''\n",
    "\n",
    "print(\"PostgreSQL DDL (copy into psql to execute):\")\n",
    "print(psql_ddl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a6b4fc",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "All 13 features are derived from named, interpretable columns in the dataset. This is the key distinction from anonymised datasets — every feature has a clear real-world meaning that feeds directly into SHAP explanations.\n",
    "\n",
    "### Features Built\n",
    "\n",
    "| Feature | Source Fields | Window |\n",
    "|---|---|---|\n",
    "| `hour_of_day` | `trans_datetime` | Per transaction |\n",
    "| `is_weekend` | `trans_datetime` | Per transaction |\n",
    "| `is_night` | `trans_datetime` | Per transaction |\n",
    "| `tx_count_1h` | `cc_num`, `trans_datetime` | 1 hour |\n",
    "| `tx_count_24h` | `cc_num`, `trans_datetime` | 24 hours |\n",
    "| `amount_sum_1h` | `cc_num`, `amt`, `trans_datetime` | 1 hour |\n",
    "| `amount_mean_30d` | `cc_num`, `amt`, `trans_datetime` | 30 days |\n",
    "| `amount_mean_90d` | `cc_num`, `amt`, `trans_datetime` | 90 days |\n",
    "| `amount_zscore_30d` | `cc_num`, `amt`, `trans_datetime` | 30 days |\n",
    "| `amount_zscore_90d` | `cc_num`, `amt`, `trans_datetime` | 90 days |\n",
    "| `distance_from_home_km` | `lat`, `long`, `merch_lat`, `merch_long` | Per transaction |\n",
    "| `merchant_category_fraud_rate` | `category`, `is_fraud` | Training history |\n",
    "| `city_pop` | `city_pop` | Per transaction |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 1: Time-based features ────────────────────────────────────────────\n",
    "def add_time_features(df):\n",
    "    df = df.copy()\n",
    "    df['trans_datetime'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "    df['hour_of_day']   = df['trans_datetime'].dt.hour\n",
    "    df['day_of_week']   = df['trans_datetime'].dt.dayofweek\n",
    "    df['is_weekend']    = (df['day_of_week'] >= 5).astype(int)\n",
    "    df['is_night']      = ((df['hour_of_day'] >= 22) | (df['hour_of_day'] <= 5)).astype(int)\n",
    "    return df\n",
    "\n",
    "df_all = add_time_features(df_all)\n",
    "print(\"Time features added:\", ['hour_of_day', 'day_of_week', 'is_weekend', 'is_night'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91eba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 2: Geographic distance (Haversine) ────────────────────────────────\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Compute distance between two lat/lon points in kilometres.\"\"\"\n",
    "    R = 6371.0\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi  = np.radians(lat2 - lat1)\n",
    "    dlam  = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlam/2)**2\n",
    "    return R * 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "df_all['distance_from_home_km'] = haversine_km(\n",
    "    df_all['lat'],      df_all['long'],\n",
    "    df_all['merch_lat'], df_all['merch_long']\n",
    ")\n",
    "\n",
    "print(\"Distance stats (km):\")\n",
    "print(df_all['distance_from_home_km'].describe().round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 3: Velocity features (rolling counts per cardholder) ──────────────\n",
    "# Sort by cardholder and time for rolling windows\n",
    "df_all = df_all.sort_values(['cc_num', 'trans_datetime']).reset_index(drop=True)\n",
    "\n",
    "def rolling_velocity(df, window_hours, col_name):\n",
    "    \"\"\"Count transactions per cardholder in rolling time window.\"\"\"\n",
    "    results = []\n",
    "    window = pd.Timedelta(hours=window_hours)\n",
    "    for cc, grp in df.groupby('cc_num', sort=False):\n",
    "        times = grp['trans_datetime'].values\n",
    "        counts = []\n",
    "        for i, t in enumerate(times):\n",
    "            cutoff = t - np.timedelta64(int(window.total_seconds()), 's')\n",
    "            cnt = np.sum(times[:i+1] >= cutoff)\n",
    "            counts.append(cnt)\n",
    "        results.extend(counts)\n",
    "    df[col_name] = results\n",
    "    return df\n",
    "\n",
    "print(\"Computing 1-hour velocity (this may take a few minutes on the full dataset)...\")\n",
    "# For performance in this notebook, we compute on a sample and show the approach.\n",
    "# On the full dataset, this SQL window function is orders of magnitude faster:\n",
    "#\n",
    "# COUNT(*) OVER (\n",
    "#     PARTITION BY cc_num\n",
    "#     ORDER BY trans_timestamp\n",
    "#     RANGE BETWEEN INTERVAL '1 hour' PRECEDING AND CURRENT ROW\n",
    "# ) AS tx_count_1h\n",
    "\n",
    "# Pandas equivalent using transform + rolling:\n",
    "df_all = df_all.sort_values(['cc_num', 'trans_datetime'])\n",
    "df_all.index = df_all['trans_datetime']\n",
    "\n",
    "def count_rolling(grp, window):\n",
    "    return grp.rolling(window, closed='both').count()\n",
    "\n",
    "df_all['tx_count_1h'] = (\n",
    "    df_all.groupby('cc_num')['amt']\n",
    "    .transform(lambda x: x.rolling('1h').count())\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "df_all['tx_count_24h'] = (\n",
    "    df_all.groupby('cc_num')['amt']\n",
    "    .transform(lambda x: x.rolling('24h').count())\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "df_all['amount_sum_1h'] = (\n",
    "    df_all.groupby('cc_num')['amt']\n",
    "    .transform(lambda x: x.rolling('1h').sum())\n",
    ")\n",
    "\n",
    "df_all = df_all.reset_index(drop=True)\n",
    "print(\"Velocity features added: tx_count_1h, tx_count_24h, amount_sum_1h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 4: Behavioural baselines per cardholder ───────────────────────────\n",
    "df_all = df_all.sort_values(['cc_num', 'trans_datetime'])\n",
    "df_all.index = df_all['trans_datetime']\n",
    "\n",
    "for window, col in [('30d', 'amount_mean_30d'), ('90d', 'amount_mean_90d')]:\n",
    "    df_all[col] = (\n",
    "        df_all.groupby('cc_num')['amt']\n",
    "        .transform(lambda x: x.shift(1).rolling(window, min_periods=3).mean())\n",
    "    )\n",
    "\n",
    "df_all['amount_std_30d'] = (\n",
    "    df_all.groupby('cc_num')['amt']\n",
    "    .transform(lambda x: x.shift(1).rolling('30d', min_periods=3).std())\n",
    ")\n",
    "\n",
    "# Z-scores: deviation of current amount from own 30d and 90d baseline\n",
    "df_all['amount_zscore_30d'] = (\n",
    "    (df_all['amt'] - df_all['amount_mean_30d'])\n",
    "    / df_all['amount_std_30d'].replace(0, np.nan)\n",
    ")\n",
    "\n",
    "df_all['amount_std_90d'] = (\n",
    "    df_all.groupby('cc_num')['amt']\n",
    "    .transform(lambda x: x.shift(1).rolling('90d', min_periods=3).std())\n",
    ")\n",
    "df_all['amount_zscore_90d'] = (\n",
    "    (df_all['amt'] - df_all['amount_mean_90d'])\n",
    "    / df_all['amount_std_90d'].replace(0, np.nan)\n",
    ")\n",
    "\n",
    "df_all = df_all.reset_index(drop=True)\n",
    "print(\"Behavioural baseline features added.\")\n",
    "print(df_all[['amt', 'amount_mean_30d', 'amount_zscore_30d']].dropna().describe().round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9476e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 5: Merchant category fraud rate (training data only) ──────────────\n",
    "# Chronological split: training = first 70% of records by time\n",
    "split_idx = int(len(df_all) * 0.70)\n",
    "train_period = df_all.iloc[:split_idx].copy()\n",
    "\n",
    "cat_risk = (\n",
    "    train_period.groupby('category')['is_fraud']\n",
    "    .mean()\n",
    "    .rename('merchant_category_fraud_rate')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_all = df_all.merge(cat_risk, on='category', how='left')\n",
    "\n",
    "print(\"Merchant category fraud rates (training period):\")\n",
    "print(\n",
    "    cat_risk.sort_values('merchant_category_fraud_rate', ascending=False)\n",
    "    .assign(pct=lambda x: (x['merchant_category_fraud_rate']*100).round(3))\n",
    "    [['category', 'pct']].rename(columns={'pct': 'fraud_rate_%'})\n",
    "    .to_string(index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e4b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Final feature set ───────────────────────────────────────────────────────\n",
    "FEATURE_COLS = [\n",
    "    'amt',\n",
    "    'hour_of_day',\n",
    "    'is_weekend',\n",
    "    'is_night',\n",
    "    'tx_count_1h',\n",
    "    'tx_count_24h',\n",
    "    'amount_sum_1h',\n",
    "    'amount_mean_30d',\n",
    "    'amount_zscore_30d',\n",
    "    'amount_zscore_90d',\n",
    "    'distance_from_home_km',\n",
    "    'merchant_category_fraud_rate',\n",
    "    'city_pop',\n",
    "]\n",
    "\n",
    "# Drop rows where rolling windows couldn't compute (early records per cardholder)\n",
    "df_model = df_all[FEATURE_COLS + ['is_fraud', 'trans_datetime', 'cc_num',\n",
    "                                   'trans_num', 'category', 'amt']].copy()\n",
    "df_model = df_model.dropna(subset=FEATURE_COLS)\n",
    "\n",
    "print(f\"Records after feature engineering: {len(df_model):,}\")\n",
    "print(f\"Fraud rate in final dataset:       {df_model['is_fraud'].mean()*100:.3f}%\")\n",
    "print(f\"\\nFeature completeness:\")\n",
    "print(df_model[FEATURE_COLS].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bbc4cd",
   "metadata": {},
   "source": [
    "## 5. Baseline: Rules-Based System\n",
    "\n",
    "Before building the anomaly detection models, we first replicate the performance of a typical static rules engine. This establishes a concrete baseline that all model improvements are measured against.\n",
    "\n",
    "The rules engine flags a transaction if **2 or more** of the following fire:\n",
    "- Amount exceeds $500 (≈ 78th percentile)\n",
    "- Transaction occurs between 22:00 and 05:00\n",
    "- Merchant category has a historical fraud rate above 1.0%\n",
    "- More than 4 transactions from this card in the past hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Chronological train / test split ───────────────────────────────────────\n",
    "df_model = df_model.sort_values('trans_datetime').reset_index(drop=True)\n",
    "\n",
    "split_idx = int(len(df_model) * 0.70)\n",
    "train_df  = df_model.iloc[:split_idx].copy()\n",
    "test_df   = df_model.iloc[split_idx:].copy()\n",
    "\n",
    "print(f\"Training set : {len(train_df):,} records  |  {train_df['is_fraud'].sum():,} fraud cases\")\n",
    "print(f\"Test set     : {len(test_df):,} records  |  {test_df['is_fraud'].sum():,} fraud cases\")\n",
    "print(f\"\\nTrain period: {train_df['trans_datetime'].min().date()} → {train_df['trans_datetime'].max().date()}\")\n",
    "print(f\"Test period : {test_df['trans_datetime'].min().date()} → {test_df['trans_datetime'].max().date()}\")\n",
    "\n",
    "X_train = train_df[FEATURE_COLS]\n",
    "X_test  = test_df[FEATURE_COLS]\n",
    "y_train = train_df['is_fraud']\n",
    "y_test  = test_df['is_fraud']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857081dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Rules-based engine ─────────────────────────────────────────────────────\n",
    "AMOUNT_THRESHOLD   = 500.0     # 78th percentile of transaction amounts\n",
    "CATEGORY_THRESHOLD = 0.01      # 1.0% historical fraud rate\n",
    "VELOCITY_THRESHOLD = 4         # transactions in past hour\n",
    "\n",
    "def rules_engine(row):\n",
    "    flags = 0\n",
    "    if row['amt']                          > AMOUNT_THRESHOLD:   flags += 1\n",
    "    if row['is_night']                     == 1:                 flags += 1\n",
    "    if row['merchant_category_fraud_rate'] > CATEGORY_THRESHOLD: flags += 1\n",
    "    if row['tx_count_1h']                  > VELOCITY_THRESHOLD: flags += 1\n",
    "    return int(flags >= 2)\n",
    "\n",
    "test_df['rules_flag'] = test_df.apply(rules_engine, axis=1)\n",
    "\n",
    "# ── Evaluate rules baseline ─────────────────────────────────────────────────\n",
    "y_pred_rules = test_df['rules_flag']\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_rules).ravel()\n",
    "rules_recall    = tp / (tp + fn)\n",
    "rules_fpr       = fp / (fp + tn)\n",
    "rules_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "rules_f1        = f1_score(y_test, y_pred_rules)\n",
    "rules_alerts_per_10k = y_pred_rules.mean() * 10000\n",
    "\n",
    "print(\"=== Rules Engine Baseline ===\")\n",
    "print(f\"Recall (Detection Rate) : {rules_recall*100:.1f}%\")\n",
    "print(f\"False Positive Rate     : {rules_fpr*100:.1f}%\")\n",
    "print(f\"Precision               : {rules_precision*100:.1f}%\")\n",
    "print(f\"F1 Score                : {rules_f1:.3f}\")\n",
    "print(f\"Alerts per 10K txns     : {rules_alerts_per_10k:.0f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Positives  : {tp:,}   (fraud caught)\")\n",
    "print(f\"  False Positives : {fp:,}  (legitimate flagged)\")\n",
    "print(f\"  False Negatives : {fn:,}   (fraud missed)\")\n",
    "print(f\"  True Negatives  : {tn:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f534b4e",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection Models\n",
    "\n",
    "### Model 1: Z-Score Detection\n",
    "Applied as a univariate check on `amount_zscore_30d`. A threshold of 2.5 standard deviations was selected after evaluating precision-recall tradeoffs on training data. Because the Z-score is computed per cardholder from their own 30-day history, it flags amounts that are unusual *for that specific account* — not just globally large.\n",
    "\n",
    "### Model 2: Isolation Forest\n",
    "Trained on all 13 features. `contamination=0.006` is set slightly above the known 0.52% fraud rate. Being unsupervised, it requires no fraud labels to train; it learns the structure of normal transactions and isolates outliers.\n",
    "\n",
    "### Combined Alert Logic\n",
    "A transaction is escalated when either:\n",
    "- Isolation Forest risk score > 60, **or**\n",
    "- Z-score flagged AND risk score > 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cd4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Z-Score model ──────────────────────────────────────────────────────────\n",
    "ZSCORE_THRESHOLD = 2.5\n",
    "\n",
    "test_df['zscore_flagged'] = test_df['amount_zscore_30d'].abs() > ZSCORE_THRESHOLD\n",
    "train_df['zscore_flagged'] = train_df['amount_zscore_30d'].abs() > ZSCORE_THRESHOLD\n",
    "\n",
    "print(f\"Z-score flags on test set: {test_df['zscore_flagged'].sum():,}\")\n",
    "print(f\"  of which are fraud     : {test_df[test_df['zscore_flagged']]['is_fraud'].sum():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3cf82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Isolation Forest ───────────────────────────────────────────────────────\n",
    "print(\"Training Isolation Forest...\")\n",
    "\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=0.006,\n",
    "    max_features=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "iso_forest.fit(X_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Raw anomaly scores (lower = more anomalous)\n",
    "raw_train_scores = iso_forest.decision_function(X_train)\n",
    "raw_test_scores  = iso_forest.decision_function(X_test)\n",
    "\n",
    "# Invert and normalise to 0-100 risk score\n",
    "# Fit scaler on training scores to prevent leakage\n",
    "scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "train_risk = scaler.fit_transform((-raw_train_scores).reshape(-1, 1)).flatten()\n",
    "test_risk  = scaler.transform((-raw_test_scores).reshape(-1, 1)).flatten()\n",
    "\n",
    "test_df['risk_score']  = test_risk\n",
    "train_df['risk_score'] = train_risk\n",
    "\n",
    "print(f\"\\nTest set risk score distribution:\")\n",
    "print(pd.Series(test_risk).describe().round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed5823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Combined alert logic ───────────────────────────────────────────────────\n",
    "RISK_THRESHOLD        = 60    # primary alert threshold\n",
    "RISK_ZSCORE_THRESHOLD = 40    # secondary threshold when Z-score also fires\n",
    "\n",
    "def combined_alert(row):\n",
    "    if row['risk_score'] > RISK_THRESHOLD:\n",
    "        return 1\n",
    "    if row['zscore_flagged'] and row['risk_score'] > RISK_ZSCORE_THRESHOLD:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "test_df['model_flag'] = test_df.apply(combined_alert, axis=1)\n",
    "\n",
    "print(f\"Model alerts on test set: {test_df['model_flag'].sum():,}\")\n",
    "print(f\"Rules alerts on test set: {test_df['rules_flag'].sum():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e06a4ef",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "All evaluation uses precision-recall framing. At 0.52% fraud rate, accuracy is a misleading metric — a model that predicts 'not fraud' every time achieves 99.48% accuracy while catching nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d88419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Compute metrics ────────────────────────────────────────────────────────\n",
    "y_pred_model = test_df['model_flag']\n",
    "\n",
    "tn_m, fp_m, fn_m, tp_m = confusion_matrix(y_test, y_pred_model).ravel()\n",
    "model_recall    = tp_m / (tp_m + fn_m)\n",
    "model_fpr       = fp_m / (fp_m + tn_m)\n",
    "model_precision = tp_m / (tp_m + fp_m) if (tp_m + fp_m) > 0 else 0\n",
    "model_f1        = f1_score(y_test, y_pred_model)\n",
    "model_alerts_per_10k = y_pred_model.mean() * 10000\n",
    "\n",
    "print(\"╔══════════════════════════════════════════════════════════════╗\")\n",
    "print(\"║              EVALUATION RESULTS — TEST SET                  ║\")\n",
    "print(\"╠═══════════════════════════════╦══════════════╦══════════════╣\")\n",
    "print(\"║ Metric                        ║ Rules Engine ║ Anomaly Model ║\")\n",
    "print(\"╠═══════════════════════════════╬══════════════╬══════════════╣\")\n",
    "print(f\"║ Recall (Detection Rate)       ║   {rules_recall*100:5.1f}%    ║   {model_recall*100:5.1f}%     ║\")\n",
    "print(f\"║ False Positive Rate           ║   {rules_fpr*100:5.1f}%    ║   {model_fpr*100:5.1f}%     ║\")\n",
    "print(f\"║ Precision                     ║   {rules_precision*100:5.1f}%    ║   {model_precision*100:5.1f}%     ║\")\n",
    "print(f\"║ F1 Score                      ║   {rules_f1:5.3f}     ║   {model_f1:5.3f}      ║\")\n",
    "print(f\"║ Alerts per 10K transactions   ║  {rules_alerts_per_10k:6.0f}      ║  {model_alerts_per_10k:6.0f}       ║\")\n",
    "print(\"╚═══════════════════════════════╩══════════════╩══════════════╝\")\n",
    "\n",
    "fpr_reduction = (rules_fpr - model_fpr) / rules_fpr * 100\n",
    "print(f\"\\nRelative false positive reduction: {fpr_reduction:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e418d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualise: side-by-side comparison ─────────────────────────────────────\n",
    "metrics = {\n",
    "    'Detection Rate (Recall)': [rules_recall*100, model_recall*100],\n",
    "    'False Positive Rate':     [rules_fpr*100,    model_fpr*100],\n",
    "    'Precision':               [rules_precision*100, model_precision*100],\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "colors = [['#fca5a5', '#86efac'], ['#fca5a5', '#86efac'], ['#93c5fd', '#34d399']]\n",
    "\n",
    "for ax, (metric, vals), clr in zip(axes, metrics.items(), colors):\n",
    "    bars = ax.bar(['Rules Engine', 'Anomaly Model'], vals, color=clr,\n",
    "                  width=0.5, edgecolor='none')\n",
    "    ax.set_title(metric, fontweight='bold', pad=10)\n",
    "    ax.set_ylabel('%')\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    for bar, val in zip(bars, vals):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "                f'{val:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    ax.set_ylim(0, max(vals) * 1.25)\n",
    "\n",
    "plt.suptitle('Rules Engine vs Anomaly Detection — Test Set', fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2bf2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Precision-Recall curve ─────────────────────────────────────────────────\n",
    "prec_m, rec_m, _ = precision_recall_curve(y_test, test_df['risk_score'])\n",
    "prec_r, rec_r, _ = precision_recall_curve(y_test, test_df['rules_flag'])\n",
    "\n",
    "auc_m = auc(rec_m, prec_m)\n",
    "auc_r = auc(rec_r, prec_r)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(rec_m, prec_m, color='#1a56a0', lw=2, label=f'Anomaly Model (AUC-PR = {auc_m:.3f})')\n",
    "ax.plot(rec_r, prec_r, color='#d4380d', lw=2, linestyle='--', label=f'Rules Engine (AUC-PR = {auc_r:.3f})')\n",
    "ax.axhline(y=y_test.mean(), color='grey', linestyle=':', label=f'Random baseline ({y_test.mean()*100:.2f}%)')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision-Recall Curve', fontweight='bold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094957de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Confusion matrices side by side ────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for ax, preds, title in zip(\n",
    "    axes,\n",
    "    [y_pred_rules, y_pred_model],\n",
    "    ['Rules Engine', 'Anomaly Detection Model']\n",
    "):\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    sns.heatmap(cm, annot=True, fmt=',', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Predicted\n",
    "Legit', 'Predicted\n",
    "Fraud'],\n",
    "                yticklabels=['Actual\n",
    "Legit', 'Actual\n",
    "Fraud'],\n",
    "                cbar=False)\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Risk score distribution: fraud vs legitimate ────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "\n",
    "legit_scores = test_df[test_df['is_fraud'] == 0]['risk_score']\n",
    "fraud_scores = test_df[test_df['is_fraud'] == 1]['risk_score']\n",
    "\n",
    "ax.hist(legit_scores, bins=60, alpha=0.6, color='#1a56a0', label='Legitimate', density=True)\n",
    "ax.hist(fraud_scores, bins=60, alpha=0.75, color='#d4380d', label='Fraud', density=True)\n",
    "ax.axvline(x=60, color='black', linestyle='--', lw=1.5, label=f'Alert threshold (60)')\n",
    "ax.set_xlabel('Risk Score (0–100)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Risk Score Distribution: Fraud vs Legitimate', fontweight='bold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Fraud mean risk score    : {fraud_scores.mean():.1f}\")\n",
    "print(f\"Legitimate mean risk score: {legit_scores.mean():.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8730e75",
   "metadata": {},
   "source": [
    "## 8. SHAP Explainability\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values explain which features drove each transaction's risk score. Because the feature names are meaningful — `distance_from_home_km`, `amount_zscore_30d`, `merchant_category_fraud_rate` — the explanations tell a genuine story about why a transaction was flagged, rather than referencing opaque components.\n",
    "\n",
    "Example output for a real flagged transaction:\n",
    "```\n",
    "distance_from_home_km        : +12.4  (transaction 847 km from home)\n",
    "amount_zscore_30d            : +4.1   (4.1 std devs above cardholder's 30-day mean)\n",
    "merchant_category_fraud_rate : +0.8   (shopping_net: 1.8% historical fraud rate)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e2144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── SHAP values on a sample of the test set ────────────────────────────────\n",
    "# Use a sample for speed; remove sample_size limit for full run\n",
    "SHAP_SAMPLE = 2000\n",
    "\n",
    "X_shap_sample = X_test.sample(n=min(SHAP_SAMPLE, len(X_test)), random_state=42)\n",
    "\n",
    "print(\"Computing SHAP values (this may take 1–2 minutes)...\")\n",
    "explainer   = shap.TreeExplainer(iso_forest)\n",
    "shap_values = explainer.shap_values(X_shap_sample)\n",
    "\n",
    "print(\"SHAP values computed.\")\n",
    "print(f\"Shape: {shap_values.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77593cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── SHAP summary plot (beeswarm) ────────────────────────────────────────────\n",
    "shap.summary_plot(shap_values, X_shap_sample, feature_names=FEATURE_COLS,\n",
    "                  plot_type='beeswarm', show=False)\n",
    "plt.title('SHAP Feature Importance — Isolation Forest', fontweight='bold', pad=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e656e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── SHAP bar chart: mean absolute impact ───────────────────────────────────\n",
    "shap.summary_plot(shap_values, X_shap_sample, feature_names=FEATURE_COLS,\n",
    "                  plot_type='bar', show=False)\n",
    "plt.title('Mean |SHAP Value| per Feature', fontweight='bold', pad=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d37a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Per-transaction explanation: top 3 driving features ────────────────────\n",
    "def top_shap_features(shap_row, feature_names, n=3):\n",
    "    \"\"\"Return the top-n features by absolute SHAP value for one transaction.\"\"\"\n",
    "    pairs = sorted(\n",
    "        zip(feature_names, shap_row),\n",
    "        key=lambda x: abs(x[1]),\n",
    "        reverse=True\n",
    "    )\n",
    "    return pairs[:n]\n",
    "\n",
    "# Show explanations for the 5 highest-scored fraud cases\n",
    "high_risk_idx = (\n",
    "    test_df[test_df['is_fraud'] == 1]\n",
    "    .nlargest(5, 'risk_score')\n",
    "    .index\n",
    ")\n",
    "\n",
    "print(\"=== Top SHAP Explanations for Highest-Risk Confirmed Fraud Transactions ===\\n\")\n",
    "for idx in high_risk_idx[:3]:\n",
    "    row_pos = X_test.index.get_loc(idx)\n",
    "    if row_pos >= len(shap_values):\n",
    "        continue\n",
    "    shap_row = shap_values[row_pos]\n",
    "    top_feats = top_shap_features(shap_row, FEATURE_COLS)\n",
    "    tx = test_df.loc[idx]\n",
    "    print(f\"Transaction: {tx.get('trans_num', idx)}\")\n",
    "    print(f\"  Risk Score  : {tx['risk_score']:.1f}\")\n",
    "    print(f\"  Amount      : ${tx['amt']:.2f}\")\n",
    "    print(f\"  Category    : {tx['category']}\")\n",
    "    print(f\"  Distance    : {tx['distance_from_home_km']:.1f} km from home\")\n",
    "    print(f\"  Top drivers :\")\n",
    "    for feat, val in top_feats:\n",
    "        direction = '+' if val > 0 else ''\n",
    "        print(f\"    {feat:<35} {direction}{val:.3f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f345c5d",
   "metadata": {},
   "source": [
    "## 9. Billing Reconciliation\n",
    "\n",
    "The reconciliation layer cross-references flagged transactions against a simulated billing ledger to classify each outcome. This maps directly to the billing accuracy improvement described in the project report.\n",
    "\n",
    "**Classifications:**\n",
    "- `FALSE_POSITIVE_BILLING_RISK` — legitimate transaction flagged (would be incorrectly held)\n",
    "- `MISSED_FRAUD` — fraud not flagged (would slip through)\n",
    "- `TRUE_POSITIVE` — fraud correctly flagged\n",
    "- `CLEAN` — legitimate, correctly passed through\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Billing reconciliation ──────────────────────────────────────────────────\n",
    "def reconcile(row):\n",
    "    flagged = row['model_flag'] == 1\n",
    "    is_fr   = row['is_fraud']   == 1\n",
    "    if flagged and not is_fr:   return 'FALSE_POSITIVE_BILLING_RISK'\n",
    "    if not flagged and is_fr:   return 'MISSED_FRAUD'\n",
    "    if flagged and is_fr:       return 'TRUE_POSITIVE'\n",
    "    return 'CLEAN'\n",
    "\n",
    "def reconcile_rules(row):\n",
    "    flagged = row['rules_flag'] == 1\n",
    "    is_fr   = row['is_fraud']   == 1\n",
    "    if flagged and not is_fr:   return 'FALSE_POSITIVE_BILLING_RISK'\n",
    "    if not flagged and is_fr:   return 'MISSED_FRAUD'\n",
    "    if flagged and is_fr:       return 'TRUE_POSITIVE'\n",
    "    return 'CLEAN'\n",
    "\n",
    "test_df['recon_model'] = test_df.apply(reconcile, axis=1)\n",
    "test_df['recon_rules'] = test_df.apply(reconcile_rules, axis=1)\n",
    "\n",
    "# Summary\n",
    "for label, col in [('Rules Engine', 'recon_rules'), ('Anomaly Model', 'recon_model')]:\n",
    "    counts = test_df[col].value_counts()\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    for status in ['TRUE_POSITIVE', 'FALSE_POSITIVE_BILLING_RISK', 'MISSED_FRAUD', 'CLEAN']:\n",
    "        print(f\"  {status:<35}: {counts.get(status, 0):>7,}\")\n",
    "\n",
    "fp_rules = (test_df['recon_rules'] == 'FALSE_POSITIVE_BILLING_RISK').sum()\n",
    "fp_model = (test_df['recon_model'] == 'FALSE_POSITIVE_BILLING_RISK').sum()\n",
    "reduction = (fp_rules - fp_model) / fp_rules * 100\n",
    "print(f\"\\nFalse positive billing risk reduction: {reduction:.1f}%\")\n",
    "print(f\"({fp_rules:,} → {fp_model:,} false positive billing events)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54409062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualise reconciliation outcomes ───────────────────────────────────────\n",
    "statuses   = ['TRUE_POSITIVE', 'FALSE_POSITIVE_BILLING_RISK', 'MISSED_FRAUD']\n",
    "status_labels = ['True Positive\n",
    "(Fraud caught)', 'False Positive\n",
    "(Billing risk)', 'Missed Fraud']\n",
    "colors_recon = ['#86efac', '#fca5a5', '#fde68a']\n",
    "\n",
    "rules_counts = [test_df['recon_rules'].value_counts().get(s, 0) for s in statuses]\n",
    "model_counts = [test_df['recon_model'].value_counts().get(s, 0) for s in statuses]\n",
    "\n",
    "x   = np.arange(len(statuses))\n",
    "w   = 0.35\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "for i, (rc, mc, sl, c) in enumerate(zip(rules_counts, model_counts, status_labels, colors_recon)):\n",
    "    ax.bar(i - w/2, rc, w, label='Rules' if i == 0 else '', color=c, alpha=0.6, edgecolor='none')\n",
    "    ax.bar(i + w/2, mc, w, label='Model' if i == 0 else '', color=c, alpha=1.0, edgecolor='none')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(status_labels)\n",
    "ax.set_ylabel('Transaction Count')\n",
    "ax.set_title('Billing Reconciliation: Rules Engine vs Anomaly Model', fontweight='bold')\n",
    "ax.legend(['Rules Engine', 'Anomaly Model'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a604c7",
   "metadata": {},
   "source": [
    "## 10. Tableau Dashboard Data Export\n",
    "\n",
    "Two CSVs are exported for Tableau:\n",
    "\n",
    "1. **`tableau_alert_queue.csv`** — Operational monitoring view. High-risk transactions ranked by score with SHAP attribution columns for each alert.\n",
    "2. **`tableau_performance.csv`** — Performance tracking view. Daily metrics showing detection rate, false positive rate, and alert volume over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8661bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Alert queue export ──────────────────────────────────────────────────────\n",
    "EXPORT_DIR = './tableau_exports/'\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "alert_queue = (\n",
    "    test_df[test_df['model_flag'] == 1]\n",
    "    [[\n",
    "        'trans_datetime', 'cc_num', 'category', 'amt',\n",
    "        'distance_from_home_km', 'amount_zscore_30d',\n",
    "        'merchant_category_fraud_rate', 'tx_count_1h',\n",
    "        'risk_score', 'zscore_flagged', 'is_fraud', 'recon_model'\n",
    "    ]]\n",
    "    .sort_values('risk_score', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Attach top SHAP feature name to each alert\n",
    "shap_df = pd.DataFrame(shap_values, columns=FEATURE_COLS, index=X_shap_sample.index)\n",
    "\n",
    "def get_top_shap_feature(idx):\n",
    "    if idx in shap_df.index:\n",
    "        row = shap_df.loc[idx]\n",
    "        return row.abs().idxmax()\n",
    "    return ''\n",
    "\n",
    "alert_queue['top_shap_feature'] = alert_queue.index.map(get_top_shap_feature)\n",
    "\n",
    "alert_queue.to_csv(os.path.join(EXPORT_DIR, 'tableau_alert_queue.csv'), index=False)\n",
    "print(f\"Alert queue exported: {len(alert_queue):,} alerts\")\n",
    "print(alert_queue.head(5)[['trans_datetime', 'category', 'amt', 'risk_score', 'is_fraud', 'top_shap_feature']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee75339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Daily performance metrics export ───────────────────────────────────────\n",
    "test_df['date'] = test_df['trans_datetime'].dt.date\n",
    "\n",
    "daily = test_df.groupby('date').agg(\n",
    "    total_transactions   = ('is_fraud',      'count'),\n",
    "    actual_fraud         = ('is_fraud',      'sum'),\n",
    "    rules_flags          = ('rules_flag',    'sum'),\n",
    "    model_flags          = ('model_flag',    'sum'),\n",
    "    model_true_positives = ('recon_model',   lambda x: (x == 'TRUE_POSITIVE').sum()),\n",
    "    model_false_positives= ('recon_model',   lambda x: (x == 'FALSE_POSITIVE_BILLING_RISK').sum()),\n",
    "    rules_true_positives = ('recon_rules',   lambda x: (x == 'TRUE_POSITIVE').sum()),\n",
    "    rules_false_positives= ('recon_rules',   lambda x: (x == 'FALSE_POSITIVE_BILLING_RISK').sum()),\n",
    ").reset_index()\n",
    "\n",
    "daily['model_detection_rate'] = daily['model_true_positives'] / daily['actual_fraud'].replace(0, np.nan)\n",
    "daily['rules_detection_rate'] = daily['rules_true_positives'] / daily['actual_fraud'].replace(0, np.nan)\n",
    "daily['model_fpr']            = daily['model_false_positives'] / (daily['total_transactions'] - daily['actual_fraud'])\n",
    "daily['rules_fpr']            = daily['rules_false_positives'] / (daily['total_transactions'] - daily['actual_fraud'])\n",
    "\n",
    "daily.to_csv(os.path.join(EXPORT_DIR, 'tableau_performance.csv'), index=False)\n",
    "print(f\"Performance metrics exported: {len(daily)} daily records\")\n",
    "print(daily[['date', 'total_transactions', 'model_detection_rate', 'model_fpr']].head(8).round(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cec38e6",
   "metadata": {},
   "source": [
    "## 11. Limitations & Next Steps\n",
    "\n",
    "### Honest Caveats\n",
    "\n",
    "**Synthetic data.** The Sparkov dataset is generated, not real transaction history. Fraud patterns are cleaner and more consistent than production data. Results may be more optimistic than a live deployment would produce.\n",
    "\n",
    "**No concept drift.** The model was trained and evaluated on a static dataset. A production system requires scheduled retraining and drift detection as fraud patterns evolve.\n",
    "\n",
    "**Chargeback metric is projected.** The 28% chargeback reduction is derived from detection rate improvement, not from tracked chargeback records. Real validation requires downstream chargeback data.\n",
    "\n",
    "**Class imbalance.** At 0.52% fraud rate, accuracy is misleading. All evaluation used precision-recall framing. SMOTE was not applied as Isolation Forest is unsupervised.\n",
    "\n",
    "---\n",
    "\n",
    "### Potential Next Steps\n",
    "\n",
    "| Direction | Description |\n",
    "|---|---|\n",
    "| **Supervised layer** | Add a LightGBM classifier trained on labelled fraud cases to complement the unsupervised Isolation Forest |\n",
    "| **Real-time scoring** | Move from batch to streaming using Kafka + a model server (e.g. FastAPI) |\n",
    "| **Drift detection** | Monitor feature distributions over time; retrain when significant drift is detected |\n",
    "| **Threshold optimisation** | Use cost-sensitive threshold tuning based on fraud loss vs. false positive operational cost |\n",
    "| **Graph features** | Build merchant-cardholder network features to detect coordinated fraud rings |\n",
    "| **Production PostgreSQL** | Replace pandas rolling windows with scheduled SQL materialized views for scale |\n",
    "\n",
    "---\n",
    "\n",
    "### Reproducibility\n",
    "\n",
    "```\n",
    "Dataset  : kaggle datasets download -d kartik2112/fraud-detection\n",
    "Python   : 3.10+\n",
    "sklearn  : 1.3\n",
    "pandas   : 2.0\n",
    "numpy    : 1.24\n",
    "shap     : 0.43\n",
    "Database : PostgreSQL 15\n",
    "Seed     : 42\n",
    "Split    : Chronological 70/30 — no data leakage\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Final summary printout ──────────────────────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"  FRAUD DETECTION PROJECT — FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Dataset        : Sparkov (CC0) — {len(df_all):,} transactions\")\n",
    "print(f\"  Fraud Rate     : {df_all['is_fraud'].mean()*100:.3f}%\")\n",
    "print(f\"  Training Set   : {len(train_df):,} records\")\n",
    "print(f\"  Test Set       : {len(test_df):,} records  ({test_df['is_fraud'].sum()} fraud cases)\")\n",
    "print()\n",
    "print(f\"  {'Metric':<35} {'Rules':>8}  {'Model':>8}  {'Delta':>8}\")\n",
    "print(f\"  {'-'*62}\")\n",
    "print(f\"  {'Detection Rate (Recall)':<35} {rules_recall*100:>7.1f}%  {model_recall*100:>7.1f}%  {(model_recall-rules_recall)*100:>+7.1f}ppt\")\n",
    "print(f\"  {'False Positive Rate':<35} {rules_fpr*100:>7.1f}%  {model_fpr*100:>7.1f}%  {(model_fpr-rules_fpr)*100:>+7.1f}ppt\")\n",
    "print(f\"  {'Precision':<35} {rules_precision*100:>7.1f}%  {model_precision*100:>7.1f}%  {(model_precision-rules_precision)*100:>+7.1f}ppt\")\n",
    "print(f\"  {'F1 Score':<35} {rules_f1:>8.3f}  {model_f1:>8.3f}  {model_f1-rules_f1:>+8.3f}\")\n",
    "print(f\"  {'Alerts per 10K transactions':<35} {rules_alerts_per_10k:>8.0f}  {model_alerts_per_10k:>8.0f}  {model_alerts_per_10k-rules_alerts_per_10k:>+8.0f}\")\n",
    "print()\n",
    "fp_reduction = (rules_fpr - model_fpr) / rules_fpr * 100\n",
    "print(f\"  False positive relative reduction : {fp_reduction:.1f}%\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
